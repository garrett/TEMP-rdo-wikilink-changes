<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8 lt-ie7" lang="en-us"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8" lang="en-us"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if IE 9]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if lt IE 10]> <html class="no-js lt-ie10" lang="en-us"> <![endif]-->
<!--[if !IE]> > <![endif]-->
<html class='no-js' lang='en'>
<!-- <![endif] -->
<head>
<title>
Load Balance OpenStack API &mdash;
RDO
</title>
<meta charset='utf-8'>
<meta content='' name='description'>
<meta content='' name='author'>
<meta content='initial-scale=1.0,user-scalable=no,maximum-scale=1,width=device-width' name='viewport'>

<link href='/images/favicon.ico' rel='shortcut icon'>
<link href='/images/apple-touch-icon-precomposed.png' rel='apple-touch-icon-precomposed'>
<link href='/images/apple-touch-icon-57x57-precomposed.png' rel='apple-touch-icon-precomposed' sizes='57x57'>
<link href='/images/apple-touch-icon-72x72-precomposed.png' rel='apple-touch-icon-precomposed' sizes='72x72'>
<link href='/images/apple-touch-icon-114x114-precomposed.png' rel='apple-touch-icon-precomposed' sizes='114x114'>
<link href="/stylesheets/application.css" rel="stylesheet" type="text/css" />
<link href="/stylesheets/print.css" rel="stylesheet" type="text/css" media="print" />
</head>
<body class=' source-md'>
<header class='masthead hidden-print' id='branding' role='banner'>
<a href='https://github.com/redhat-openstack/website/edit/master/source/api/load-balance-openstack-api.html.md'>
<img alt='Edit on GitHub' data-canonical-src='/images/edit.png' src='/images/edit.png' style='position: absolute; top: 0; right: 0; border: 0;'>
</a>
<section class='hgroup'>
<h1>
<a href="/"><img id="logo" class="logo " alt="RDO" src="/images/rdo-logo-white.png" />
</a></h1>
</section>
<div id='access'>
<nav role='navigation'>
<ul class='nav nav-pills'>
<li class='nav-link-home' role='menuitem'>
<a href='/'>Home</a>
</li>

<li class='nav-link-blog' role='menuitem'>
<a href='/blog/'>Blog</a>
</li>

<li class='nav-link-events' role='menuitem'>
<a href='/events/'>Events</a>
</li>

<li class='nav-link-community' role='menuitem'>
<a href='/community/'>Community</a>
</li>

<li class='nav-link-docs' role='menuitem'>
<a href='/documentation/'>Docs</a>
</li>

<li class='nav-link-search' role='menuitem'>
<a href='/search/'>Search</a>
</li>

</ul>
</nav>

</div>
</header>

<section class='page-wrap' id='page-wrap'>
<section class='page' id='page'>
<section class='container content' id='content'>

<!--[if lt IE 7]>
<p class="chromeframe">You are using an outdated browser.
<a href="http://browsehappy.com/">Upgrade your browser today</a> or
<a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
<![endif]-->

<h1 id="load-balancing-openstack-api-services">Load-Balancing OpenStack API Services</h1>

<p>When running each of the OpenStack API services on a single controller node, there exists not only a single point of failure but also a potential bottleneck. This guide will show how to manually deploy additional OpenStack controller nodes and configure HAProxy to load-balance each OpenStack API service. In addition, keepalived will be used to provide high availability for the HAProxy load-balancer.</p>

<h2 id="prerequisites">Prerequisites</h2>

<p>This guide assumes that OpenStack has been deployed on a single node via the packstack installer. See the RDO QuickStart guide for more information.</p>

<h2 id="overview">Overview</h2>

<p>The general approach is to install and configure the load-balancer nodes, followed by deploying the additional OpenStack controller nodes. This may seem backwards, but it is better to configure our OpenStack controller nodes after our virtual IP address is active.</p>

<p>Note that in the example given in this guide the load-balancer is not co-located on the OpenStack controller nodes. While it may be possible to merge the load-balancer and controller nodes, it requires extra care to ensure that HAProxy and the various OpenStack API services are not listening on the same ports.</p>

<h2 id="configure-load-balancer-nodes">Configure Load-Balancer Nodes</h2>

<p>The first step is select two or more nodes that will serve as load-balancer nodes. Each of these nodes will run both haproxy and keepalived. In our example there will be three load-balancer nodes.</p>

<p>Next, identify which nodes will serve as OpenStack controller nodes. This includes both the single-node OpenStack deployment and at least one additional controller node to be configured. Each of these nodes will run the OpenStack API services. In our example there will be three OpenStack controller nodes with the following IP addresses:</p>

<pre class="highlight plaintext"><code>10.15.85.141&#x000A;10.15.85.142&#x000A;10.15.85.143&#x000A;</code></pre>

<p>Finally, select a virtual IP address. This can be any available IP address on the same subnet as our load-balancer and controller nodes. The example described in this guide will use the following virtual IP address:</p>

<pre class="highlight plaintext"><code>10.15.85.31&#x000A;</code></pre>

<p>Login to each of the load-balancer nodes and install both keepalived and haproxy.</p>

<pre class="highlight plaintext"><code># yum install keepalived haproxy&#x000A;</code></pre>

<h3 id="haproxy">HAProxy</h3>

<p>To configure the load-balancer, begin by editing the HAProxy configuration file /etc/haproxy/haproxy.cfg. In the example below shows a very simple haproxy.cfg file that defines a proxy for each OpenStack API service. Each proxy has a frontend and a backend. The frontend will define the IP address and port on which HAProxy will listen, as well that the default backend. A proxy's backend defines the pool of servers which traffic will be load-balanced across using a specific algorithm.</p>

<p>Note that HAProxy has many powerful options. The example shown below is intended to be a very basic configuration. Please refer to the official HAProxy documentation for an exhaustive list of options.</p>

<pre class="highlight plaintext"><code>global&#x000A;    daemon&#x000A;&#x000A;defaults&#x000A;    mode http&#x000A;    maxconn 10000&#x000A;    timeout connect 10s&#x000A;    timeout client 10s&#x000A;    timeout server 10s&#x000A;&#x000A;frontend keystone-admin-vip&#x000A;    bind 10.15.85.31:35357&#x000A;    default_backend keystone-admin-api&#x000A;&#x000A;frontend keystone-public-vip&#x000A;    bind 10.15.85.31:5000&#x000A;    default_backend keystone-public-api&#x000A;&#x000A;frontend quantum-vip&#x000A;    bind 10.15.85.31:9696&#x000A;    default_backend quantum-api&#x000A;&#x000A;frontend glance-vip&#x000A;    bind 10.15.85.31:9191&#x000A;    default_backend glance-api&#x000A;&#x000A;frontend glance-registry-vip&#x000A;    bind 10.15.85.31:9292&#x000A;    default_backend glance-registry-api&#x000A;&#x000A;frontend nova-ec2-vip&#x000A;    bind 10.15.85.31:8773&#x000A;    default_backend nova-ec2-api&#x000A;&#x000A;frontend nova-compute-vip&#x000A;    bind 10.15.85.31:8774&#x000A;    default_backend nova-compute-api&#x000A;&#x000A;frontend nova-metadata-vip&#x000A;    bind 10.15.85.31:8775&#x000A;    default_backend nova-metadata-api&#x000A;&#x000A;frontend cinder-vip&#x000A;    bind 10.15.85.31:8776&#x000A;    default_backend cinder-api&#x000A;&#x000A;backend keystone-admin-api&#x000A;    balance roundrobin&#x000A;    server mesa-virt-01 10.15.85.141:35357 check inter 10s&#x000A;    server mesa-virt-02 10.15.85.142:35357 check inter 10s&#x000A;    server mesa-virt-03 10.15.85.143:35357 check inter 10s&#x000A;&#x000A;backend keystone-public-api&#x000A;    balance roundrobin&#x000A;    server mesa-virt-01 10.15.85.141:5000 check inter 10s&#x000A;    server mesa-virt-02 10.15.85.142:5000 check inter 10s&#x000A;    server mesa-virt-03 10.15.85.143:5000 check inter 10s&#x000A;&#x000A;backend quantum-api&#x000A;    balance roundrobin&#x000A;    server mesa-virt-01 10.15.85.141:9696 check inter 10s&#x000A;    server mesa-virt-02 10.15.85.142:9696 check inter 10s&#x000A;    server mesa-virt-03 10.15.85.143:9696 check inter 10s&#x000A;&#x000A;backend glance-api&#x000A;    balance roundrobin&#x000A;    server mesa-virt-01 10.15.85.141:9191 check inter 10s&#x000A;    server mesa-virt-02 10.15.85.142:9191 check inter 10s&#x000A;    server mesa-virt-03 10.15.85.143:9191 check inter 10s&#x000A;&#x000A;backend glance-registry-api&#x000A;    balance roundrobin&#x000A;    server mesa-virt-01 10.15.85.141:9292 check inter 10s&#x000A;    server mesa-virt-02 10.15.85.142:9292 check inter 10s&#x000A;    server mesa-virt-03 10.15.85.143:9292 check inter 10s&#x000A;&#x000A;backend nova-ec2-api&#x000A;    balance roundrobin&#x000A;    server mesa-virt-01 10.15.85.141:8773 check inter 10s&#x000A;    server mesa-virt-02 10.15.85.142:8773 check inter 10s&#x000A;    server mesa-virt-03 10.15.85.143:8773 check inter 10s&#x000A;&#x000A;backend nova-compute-api&#x000A;    balance roundrobin&#x000A;    server mesa-virt-01 10.15.85.141:8774 check inter 10s&#x000A;    server mesa-virt-02 10.15.85.142:8774 check inter 10s&#x000A;    server mesa-virt-03 10.15.85.143:8774 check inter 10s&#x000A;&#x000A;backend nova-metadata-api&#x000A;    balance roundrobin&#x000A;    server mesa-virt-01 10.15.85.141:8775 check inter 10s&#x000A;    server mesa-virt-02 10.15.85.142:8775 check inter 10s&#x000A;    server mesa-virt-03 10.15.85.143:8775 check inter 10s&#x000A;&#x000A;backend cinder-api&#x000A;    balance roundrobin&#x000A;    server mesa-virt-01 10.15.85.141:8776 check inter 10s&#x000A;    server mesa-virt-02 10.15.85.142:8776 check inter 10s&#x000A;    server mesa-virt-03 10.15.85.143:8776 check inter 10s&#x000A;</code></pre>

<p>Notice that each server declaration has "check inter 10s" appended. This instructs HAProxy to perform a health-check of the server every 10 seconds. When a server fails a health-check it is considered "down" and traffic will not be forwarded to that. However the health-check will still be performed at 10 second intervals. Should the server resume responding to health-checks, it will be considered "active" and once again be considered for load-balancing traffic.</p>

<p>The haproxy.cfg file should be identical on each of the load-balancer nodes, so copy that file to the all other load-balancer nodes.</p>

<h3 id="keepalived">Keepalived</h3>

<p>Before we start the HAProxy service on our load-balancer nodes, configure keepalived. In our example, keepalived will be used to provide high-availability for our HAProxy load-balancer. Without keepalived, HAProxy would be a single point of failure.</p>

<p>In our example, the keepalived service is used for virtual IP failover. Keepalived uses Virtual Router Redundancy Protocol (VRRP) to handle virtual IP failover. VRRP is a priority-based protocol where at any time there can be multiple backup nodes and a single master node. The master node it responsible for sending out periodic advertisements that include the node's priority. Should the backup nodes stop receiving advertisements, a new master is selected based on priority.</p>

<p>Note that VRRP advertisements are sent via multicast, so all load-balancer nodes should be capable of sending and receiving multicast traffic. Be particularly cautious of iptables rules that may block multicast traffic.</p>

<p>On each load-balancer node, edit /etc/keepalived/keepalived.conf to configure keepalived. Below is the keepalived.conf file used in our example:</p>

<pre class="highlight plaintext"><code>vrrp_script haproxy-check {&#x000A;    script "killall -0 haproxy"&#x000A;    interval 2&#x000A;    weight 10&#x000A;}&#x000A;&#x000A;vrrp_instance openstack-vip {&#x000A;    state BACKUP&#x000A;    priority 102&#x000A;    interface eth0&#x000A;    virtual_router_id 47&#x000A;    advert_int 3&#x000A;&#x000A;    virtual_ipaddress {&#x000A;        10.15.85.31&#x000A;    }&#x000A;&#x000A;    track_script {&#x000A;    haproxy-check&#x000A;    }&#x000A;}&#x000A;</code></pre>

<p>The keepalived.conf configuration file will not be the same on each load-balancer node. Specifically, each load-balancer node should have a unique priority. Also, one load-balancer node may be configured to default to MASTER state, but this is not required.</p>

<p>Notice that the keepalived configuration has a VRRP script. In our example the script is named "haproxy-check". This script is used to check that our HAProxy service is running. If this script succeeds, the node's effective priority is increased by 10. Since the example above has a base priority of 102, the effective priority for the node will be 112 as long as HAProxy is running. If HAProxy should stop running for any reason, the node's effective priority will return to 102.</p>

<p>As mentioned above, each load-balancer node should have a unique priority. It is important that the difference between each node's priority is not greater than the weight of our VRRP scipt. For example, if our three load-balancer nodes had base priorities of 102, 132, and 162 respectively, yet the weight of our VRRP script was only 10, a failed VRRP script may not reduce a node's effective priority enough to cause a failover.</p>

<p>Next, configure the load-balancer nodes to allow HAProxy to bind to a non-local IP address. This is needed since HAProxy will be running on each of our load-balancer nodes, yet keepalived will ensure that our virtual IP address exists on only one of our load-balancer nodes at any given time.</p>

<pre class="highlight plaintext"><code># sysctl -w net.ipv4.ip_nonlocal_bind=1&#x000A;</code></pre>

<p>To make this change permanent, add the following line to /etc/sysctl.conf:</p>

<pre class="highlight plaintext"><code>net.ipv4.ip_nonlocal_bind=1&#x000A;</code></pre>

<p>With both keepalived and haproxy configured on each node, enable and start each service.</p>

<pre class="highlight plaintext"><code># chkconfig haproxy on&#x000A;# chkconfig keepalived on&#x000A;# service haproxy start&#x000A;# service keepalived start&#x000A;</code></pre>

<p>At this point both services should be running on each node. The node with the highest priority should own the virtual IP address and HAProxy should be ready to load-balance traffic for our OpenStack API services to our controller nodes once they are configured.</p>

<h2 id="configure-openstack-controller-nodes">Configure OpenStack Controller Nodes</h2>

<p>The next step is to deploy additional OpenStack controller nodes and configure them to use our virtual IP address. The easiest way to do this is install the required packages on the additional controller nodes, edit the appropriate configuration files on the original single-node deployment, and finally copy the configuration files to the new controller nodes. When copying OpenStack configuration files between nodes, take care that the files have the correct owner, group and permissions.</p>

<h3 id="keystone">Keystone</h3>

<p>On the new controller nodes, install the OpenStack Keystone service:</p>

<pre class="highlight plaintext"><code># yum install openstack-keystone&#x000A;</code></pre>

<p>Copy the configuration files in the /etc/keystone/ directory on the original controller node to the additional controller nodes.</p>

<pre class="highlight plaintext"><code>/etc/keystone&#x000A;/etc/keystone/default_catalog.templates&#x000A;/etc/keystone/keystone.conf&#x000A;/etc/keystone/logging.conf&#x000A;/etc/keystone/policy.json&#x000A;/etc/keystone/ssl&#x000A;/etc/keystone/ssl/certs&#x000A;/etc/keystone/ssl/certs/01.pem&#x000A;/etc/keystone/ssl/certs/cakey.pem&#x000A;/etc/keystone/ssl/certs/ca.pem&#x000A;/etc/keystone/ssl/certs/index.txt&#x000A;/etc/keystone/ssl/certs/index.txt.attr&#x000A;/etc/keystone/ssl/certs/index.txt.old&#x000A;/etc/keystone/ssl/certs/openssl.conf&#x000A;/etc/keystone/ssl/certs/req.pem&#x000A;/etc/keystone/ssl/certs/serial&#x000A;/etc/keystone/ssl/certs/serial.old&#x000A;/etc/keystone/ssl/certs/signing_cert.pem&#x000A;/etc/keystone/ssl/private&#x000A;/etc/keystone/ssl/private/signing_key.pem&#x000A;</code></pre>

<p>On the new controller nodes, enable and start the OpenStack Keystone service:</p>

<pre class="highlight plaintext"><code># chkconfig openstack-keystone on&#x000A;# service openstack-keystone start&#x000A;</code></pre>

<p>On the original controller node, restart the OpenStack Keystone service:</p>

<pre class="highlight plaintext"><code># service openstack-keystone restart&#x000A;</code></pre>

<h3 id="quantum">Quantum</h3>

<p>On the new controller nodes, install the OpenStack Quantum service and the appropriate L2 plugin:</p>

<pre class="highlight plaintext"><code># yum install openstack-quantum&#x000A;# yum install openstack-quantum-openvswitch&#x000A;</code></pre>

<p>Since the Keystone and Nova services will also be load-balanced, make the following changes to our configuration files:</p>

<p>/etc/quantum/quantum.conf</p>

<pre class="highlight plaintext"><code>auth_host = 10.15.85.31&#x000A;</code></pre>

<p>/etc/quantum/api-paste.ini</p>

<pre class="highlight plaintext"><code>auth_host = 10.15.85.31&#x000A;</code></pre>

<p>/etc/quantum/metadata_agent.ini</p>

<pre class="highlight plaintext"><code>auth_url = http://10.15.85.31:35357/v2.0&#x000A;nova_metadata_ip = 10.15.85.31&#x000A;</code></pre>

<p>Copy the configuration files in the /etc/quantum/ directory on the original controller node to the additional controller nodes:</p>

<pre class="highlight plaintext"><code>/etc/quantum&#x000A;/etc/quantum/api-paste.ini&#x000A;/etc/quantum/dhcp_agent.ini&#x000A;/etc/quantum/l3_agent.ini&#x000A;/etc/quantum/lbaas_agent.ini&#x000A;/etc/quantum/metadata_agent.ini&#x000A;/etc/quantum/plugin.ini -&gt; /etc/quantum/plugins/openvswitch/ovs_quantum_plugin.ini&#x000A;/etc/quantum/plugins&#x000A;/etc/quantum/plugins/openvswitch&#x000A;/etc/quantum/plugins/openvswitch/ovs_quantum_plugin.ini&#x000A;/etc/quantum/policy.json&#x000A;/etc/quantum/quantum.conf&#x000A;/etc/quantum/release&#x000A;/etc/quantum/rootwrap.conf&#x000A;</code></pre>

<p>On the new controller nodes, enable and start the OpenStack Quantum service:</p>

<pre class="highlight plaintext"><code># chkconfig quantum-server on&#x000A;# service quantum-server start&#x000A;</code></pre>

<p>On the original controller node, restart the OpenStack Quantum service:</p>

<pre class="highlight plaintext"><code># service quantum-server restart&#x000A;</code></pre>

<h3 id="glance">Glance</h3>

<p>On the new controller nodes, install the OpenStack Glance service:</p>

<pre class="highlight plaintext"><code># yum install openstack-glance&#x000A;</code></pre>

<p>Make the following modifications to the configuration files:</p>

<p>/etc/glance/glance-api.conf</p>

<pre class="highlight plaintext"><code>registry_host = 10.15.85.31&#x000A;auth_host = 10.15.85.31&#x000A;</code></pre>

<p>/etc/glance/glance-registry.conf</p>

<pre class="highlight plaintext"><code>auth_host = 10.15.85.31&#x000A;</code></pre>

<p>Copy the configuration files in the /etc/glance/ directory on the original controller node to the additional controller nodes:</p>

<pre class="highlight plaintext"><code>/etc/glance&#x000A;/etc/glance/glance-api.conf&#x000A;/etc/glance/glance-api-paste.ini&#x000A;/etc/glance/glance-cache.conf&#x000A;/etc/glance/glance-registry.conf&#x000A;/etc/glance/glance-registry-paste.ini&#x000A;/etc/glance/glance-scrubber.conf&#x000A;/etc/glance/policy.json&#x000A;/etc/glance/schema-image.json&#x000A;</code></pre>

<p>On the new controller nodes, enable and start the OpenStack Glance API and registry services:</p>

<pre class="highlight plaintext"><code># chkconfig openstack-glance-registry  on&#x000A;# chkconfig openstack-glance-api on&#x000A;# service openstack-glance-registry start&#x000A;# service openstack-glance-api start&#x000A;</code></pre>

<p>On the original controller node, restart the OpenStack Glance API and registry services:</p>

<pre class="highlight plaintext"><code># service openstack-glance-registry restart&#x000A;# service openstack-glance-api restart&#x000A;</code></pre>

<h3 id="nova">Nova</h3>

<p>On the new controller nodes, install the OpenStack Nova service and the Cinder client:</p>

<pre class="highlight plaintext"><code># yum install openstack-nova-api&#x000A;# yum install python-cinderclient&#x000A;</code></pre>

<p>Make the following modifications to the configuration files:</p>

<p>/etc/nova/nova.conf</p>

<pre class="highlight plaintext"><code>metadata_host = 10.15.85.31&#x000A;quantum_admin_auth_url = http://10.15.85.31:35357/v2.0&#x000A;quantum_url = http://10.15.85.31:9696&#x000A;glance_api_servers = 10.15.85.31:9292&#x000A;</code></pre>

<p>/etc/nova/api-paste.ini</p>

<pre class="highlight plaintext"><code>auth_host = 10.15.85.31&#x000A;</code></pre>

<p>Copy the configuration files in the /etc/nova/ directory on the original controller node to the additional controller nodes:</p>

<pre class="highlight plaintext"><code>/etc/nova&#x000A;/etc/nova/api-paste.ini&#x000A;/etc/nova/nova.conf&#x000A;/etc/nova/policy.json&#x000A;/etc/nova/release&#x000A;/etc/nova/rootwrap.conf&#x000A;</code></pre>

<p>On the new controller nodes, enable and start the OpenStack Nova API service:</p>

<pre class="highlight plaintext"><code># chkconfig openstack-nova-api on&#x000A;# service openstack-nova-api start&#x000A;</code></pre>

<p>On the original controller node, restart the OpenStack Nova service:</p>

<pre class="highlight plaintext"><code># service openstack-nova-api restart&#x000A;</code></pre>

<h3 id="cinder">Cinder</h3>

<p>On the new controller nodes, install the OpenStack Cinder service:</p>

<pre class="highlight plaintext"><code># yum install openstack-cinder&#x000A;</code></pre>

<p>Make the following modifications to the configuration files:</p>

<p>/etc/cinder/cinder.conf</p>

<pre class="highlight plaintext"><code>glance_host = 10.15.85.31&#x000A;</code></pre>

<p>/etc/cinder/api-paste.ini</p>

<pre class="highlight plaintext"><code>service_host = 10.15.85.31&#x000A;auth_host = 10.15.85.31&#x000A;</code></pre>

<p>On the new controller nodes, enable and start the OpenStack Cinder service:</p>

<pre class="highlight plaintext"><code># chkconfig openstack-cinder-api on&#x000A;# service openstack-cinder-api start&#x000A;</code></pre>

<p>One the original controller node, restart the OpenStack Cinder service:</p>

<pre class="highlight plaintext"><code># service openstack-cinder-api restart&#x000A;</code></pre>

<h2 id="create-service-endpoints">Create Service Endpoints</h2>

<p>Now that the OpenStack services running on multiple controller nodes and the load-balancer nodes are listening on our virtual IP address, return to the original OpenStack node and recreate our service endpoints. This is necessary so that endpoint discovery will direct clients to the virtual IP address.</p>

<p>First, get the list of services from keystone:</p>

<pre class="highlight plaintext"><code># keystone service-list&#x000A;+----------------------------------+----------+--------------+--------------------------------+&#x000A;|                id                |   name   |     type     |          description           |&#x000A;+----------------------------------+----------+--------------+--------------------------------+&#x000A;| 79b92be20bec4a469725e8c5e2bb46a4 |  cinder  |    volume    |         Cinder Service         |&#x000A;| a959fca8b65745d485131cff9d41480f |  glance  |    image     |    Openstack Image Service     |&#x000A;| b2721815afab4c4784673029609cb84d | keystone |   identity   |   OpenStack Identity Service   |&#x000A;| aa5220a1065b4992baf5cdfe309033e6 |   nova   |   compute    |   Openstack Compute Service    |&#x000A;| 31ee714b590d41fc9833618abc349642 | nova_ec2 |     ec2      |          EC2 Service           |&#x000A;| 976a816bcb29441b882d6cc01faf29ec | quantum  |   network    |   Quantum Networking Service   |&#x000A;| 7a6df59033e54590a4e4fc8ec8225c56 |  swift   | object-store | Openstack Object-Store Service |&#x000A;| e93698a466d144a58c5323a808dbf480 | swift_s3 |      s3      |      Openstack S3 Service      |&#x000A;+----------------------------------+----------+--------------+--------------------------------+&#x000A;</code></pre>

<p>Next, get the list of service endpoint from keystone:</p>

<pre class="highlight plaintext"><code>  # keystone endpoint-list&#x000A;</code></pre>

<p>Notice that each URL for a given endpoint points to the original OpenStack controller node. In our example, this is 10.15.85.141. In order to load-balancer the OpenStack services, create a new endpoint for each service. Each URL for the new endpoints should point to the virtual IP address, which is 10.15.85.31 in our example.</p>

<h3 id="keystone-1">Keystone</h3>

<p>Create a new endpoint for the Keystone service that points to our virtual IP address:</p>

<pre class="highlight plaintext"><code># keystone endpoint-create --region RegionOne --service-id b2721815afab4c4784673029609cb84d \&#x000A;  --publicurl "http://10.15.85.31:5000/v2.0" \&#x000A;  --internalurl "http://10.15.85.31:5000/v2.0" \&#x000A;  --adminurl "http://10.15.85.31:35357/v2.0"&#x000A;+-------------+----------------------------------+&#x000A;|   Property  |              Value               |&#x000A;+-------------+----------------------------------+&#x000A;|   adminurl  |  http://10.15.85.31:35357/v2.0   |&#x000A;|      id     | c58e5c86ceba47429c401456b2ba889b |&#x000A;| internalurl |   http://10.15.85.31:5000/v2.0   |&#x000A;|  publicurl  |   http://10.15.85.31:5000/v2.0   |&#x000A;|    region   |            RegionOne             |&#x000A;|  service_id | b2721815afab4c4784673029609cb84d |&#x000A;+-------------+----------------------------------+&#x000A;</code></pre>

<p>Delete the old endpoint for the Keystone service:</p>

<pre class="highlight plaintext"><code># keystone endpoint-delete ca8676494d73487397f9223432cb8d07&#x000A;Endpoint has been deleted.&#x000A;</code></pre>

<h3 id="quantum-1">Quantum</h3>

<p>Create a new endpoint for the Quantum service that points to our virtual IP address:</p>

<pre class="highlight plaintext"><code># keystone endpoint-create --region RegionOne --service-id 976a816bcb29441b882d6cc01faf29ec \&#x000A;  --publicurl "http://10.15.85.31:9696" \&#x000A;  --internalurl "http://10.15.85.31:9696" \&#x000A;  --adminurl "http://10.15.85.31:9696"&#x000A;+-------------+----------------------------------+&#x000A;|   Property  |              Value               |&#x000A;+-------------+----------------------------------+&#x000A;|   adminurl  |     http://10.15.85.31:9696      |&#x000A;|      id     | 81c6dafcd2814b8ab43fdd2296a110ee |&#x000A;| internalurl |     http://10.15.85.31:9696      |&#x000A;|  publicurl  |     http://10.15.85.31:9696      |&#x000A;|    region   |            RegionOne             |&#x000A;|  service_id | 976a816bcb29441b882d6cc01faf29ec |&#x000A;+-------------+----------------------------------+&#x000A;</code></pre>

<p>Delete the old endpoint for the Quantum service:</p>

<pre class="highlight plaintext"><code># keystone endpoint-delete d3bc590da84042cb8f4df4296550d689&#x000A;Endpoint has been deleted.&#x000A;</code></pre>

<h3 id="glance-1">Glance</h3>

<p>Create a new endpoint for the Glance service that points to our virtual IP address:</p>

<pre class="highlight plaintext"><code># keystone endpoint-create --region RegionOne --service-id a959fca8b65745d485131cff9d41480f \&#x000A;  --publicurl "http://10.15.85.31:9292" \&#x000A;  --internalurl "http://10.15.85.31:9292" \&#x000A;  --adminurl "http://10.15.85.31:9292"&#x000A;+-------------+----------------------------------+&#x000A;|   Property  |              Value               |&#x000A;+-------------+----------------------------------+&#x000A;|   adminurl  |     http://10.15.85.31:9292      |&#x000A;|      id     | 16e5704be5cf48ff8fca4f608f72dd00 |&#x000A;| internalurl |     http://10.15.85.31:9292      |&#x000A;|  publicurl  |     http://10.15.85.31:9292      |&#x000A;|    region   |            RegionOne             |&#x000A;|  service_id | a959fca8b65745d485131cff9d41480f |&#x000A;+-------------+----------------------------------+&#x000A;</code></pre>

<p>Delete the old endpoint for the Glance service:</p>

<pre class="highlight plaintext"><code># keystone endpoint-delete f66323395c394322b2e20fee63777927&#x000A;Endpoint has been deleted.&#x000A;</code></pre>

<h3 id="nova-1">Nova</h3>

<p>Create a new endpoint for the Nova (compute) service that points to our virtual IP address:</p>

<pre class="highlight plaintext"><code># keystone endpoint-create --region RegionOne --service-id aa5220a1065b4992baf5cdfe309033e6 \&#x000A;  --publicurl "http://10.15.85.31:8774/v2/%(tenant_id)s" \&#x000A;  --internalurl "http://10.15.85.31:8774/v2/%(tenant_id)s" \&#x000A;  --adminurl "http://10.15.85.31:8774/v2/%(tenant_id)s"&#x000A;+-------------+------------------------------------------+&#x000A;|   Property  |                  Value                   |&#x000A;+-------------+------------------------------------------+&#x000A;|   adminurl  | http://10.15.85.31:8774/v2/%(tenant_id)s |&#x000A;|      id     |     898a3207f41d4f9a942193f881e11b65     |&#x000A;| internalurl | http://10.15.85.31:8774/v2/%(tenant_id)s |&#x000A;|  publicurl  | http://10.15.85.31:8774/v2/%(tenant_id)s |&#x000A;|    region   |                RegionOne                 |&#x000A;|  service_id |     aa5220a1065b4992baf5cdfe309033e6     |&#x000A;+-------------+------------------------------------------+&#x000A;</code></pre>

<p>Delete the old endpoint for the Nova (compute) service:</p>

<pre class="highlight plaintext"><code># keystone endpoint-delete f9cbe1df0eb244fbb76ac4dc4b06feb2&#x000A;Endpoint has been deleted.&#x000A;</code></pre>

<p>Create a new endpoint for the Nova (EC2) service that points to our virtual IP address:</p>

<pre class="highlight plaintext"><code># keystone endpoint-create --region RegionOne --service-id 31ee714b590d41fc9833618abc349642 \&#x000A;  --publicurl "http://10.15.85.31:8773/services/Cloud" \&#x000A;  --internalurl "http://10.15.85.31:8773/services/Cloud" \&#x000A;  --adminurl "http://10.15.85.31:8773/services/Admin"&#x000A;+-------------+----------------------------------------+&#x000A;|   Property  |                 Value                  |&#x000A;+-------------+----------------------------------------+&#x000A;|   adminurl  | http://10.15.85.31:8773/services/Admin |&#x000A;|      id     |    741b58300867430a812f485323d1bc30    |&#x000A;| internalurl | http://10.15.85.31:8773/services/Cloud |&#x000A;|  publicurl  | http://10.15.85.31:8773/services/Cloud |&#x000A;|    region   |               RegionOne                |&#x000A;|  service_id |    31ee714b590d41fc9833618abc349642    |&#x000A;+-------------+----------------------------------------+&#x000A;</code></pre>

<p>Delete the old endpoint for the Nova (EC2) service:</p>

<pre class="highlight plaintext"><code># keystone endpoint-delete b51a4af8149b479b8856db85c6ea41ba&#x000A;Endpoint has been deleted.&#x000A;</code></pre>

<h3 id="cinder-1">Cinder</h3>

<p>Create a new endpoint for the Cinder (volume) service that points to our virtual IP address:</p>

<pre class="highlight plaintext"><code># keystone endpoint-create --region RegionOne --service-id 79b92be20bec4a469725e8c5e2bb46a4 \&#x000A;  --publicurl "http://10.15.85.31:8776/v1/%(tenant_id)s" \&#x000A;  --internalurl "http://10.15.85.31:8776/v1/%(tenant_id)s" \&#x000A;  --adminurl "http://10.15.85.31:8776/v1/%(tenant_id)s"&#x000A;+-------------+------------------------------------------+&#x000A;|   Property  |                  Value                   |&#x000A;+-------------+------------------------------------------+&#x000A;|   adminurl  | http://10.15.85.31:8776/v1/%(tenant_id)s |&#x000A;|      id     |     70c27ea021044690841bca572e4023b1     |&#x000A;| internalurl | http://10.15.85.31:8776/v1/%(tenant_id)s |&#x000A;|  publicurl  | http://10.15.85.31:8776/v1/%(tenant_id)s |&#x000A;|    region   |                RegionOne                 |&#x000A;|  service_id |     79b92be20bec4a469725e8c5e2bb46a4     |&#x000A;+-------------+------------------------------------------+&#x000A;</code></pre>

<p>Delete the old endpoint for the Cinder (volume) service:</p>

<pre class="highlight plaintext"><code># keystone endpoint-delete 0b52480b89964721bc2fc59cc41ad16c&#x000A;Endpoint has been deleted.&#x000A;</code></pre>

<h2 id="conclusion">Conclusion</h2>

<p>The final step is to edit the keystone_adminrc file on the client node(s). Specifically, change the OS_AUTH_URL variable such that is points to the virtual IP address:</p>

<p>~/keystone_adminrc</p>

<pre class="highlight plaintext"><code>OS_AUTH_URL=http://10.15.85.141:35357/v2.0/&#x000A;</code></pre>

<p>Any traffic between the clients and OpenStack API services should now pass through the load-balancer to one of the backend OpenStack controller nodes.</p>

</section>
</section>
</section>
<footer class='text-center' id='footer'>
<hr class='visible-print'>
<div class='footer-rdo'>
<dl>
  <dt>Docs</dt>
  <dd><a href="/install/quickstart">Download</a></dd>
  <dd><a href="/rdo/faq">Common questions</a></dd>
  <dd><a href="/troubleshooting/">Troubleshooting</a></dd>
  <dd><a href="/rdo/">About RDO</a></dd>
</dl>

<dl>
  <dt>Use RDO</dt>
  <dd><a href="/install/quickstart">Quickstart</a></dd>
  <dd><a href="/rdo/release-cadence">Releases</a></dd>
  <dd><a href="http://trunk.rdoproject.org/">Trunk builds</a></dd>
</dl>

<dl>
  <dt>Community</dt>
  <dd><a href="/community/">Participate</a></dd>
  <dd><a href="http://ask.rdoproject.org/">Ask (Q&amp;A)</a></dd>
  <dd><a href="https://bugzilla.redhat.com/buglist.cgi?product=RDO&amp;query_format=advanced&amp;bug_status=NEW&amp;bug_status=ASSIGNED">Browse open issues</a></dd>
  <dd><a href="https://bugzilla.redhat.com/enter_bug.cgi?product=RDO">Report a problem</a></dd>
</dl>

<dl>
  <dt>Support</dt>
  <dd><a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a></dd>
  <dd><a href="https://www.redhat.com/mailman/listinfo/rdo-list">Contact us</a></dd>
</dl>

</div>

<ul class='footer-nav-list'>
<li><strong>RDO</strong> is a <strong>Red Hat</strong>-sponsored community project</li>
</ul>

<div class='copyright'>
&copy; 2016 RDO
<span class='legal'><a href="/legal/">Legal & Privacy</a></span>
</div>
<div class='edit-this-page'>
<a target="_blank" href="https://github.com/redhat-openstack/website/edit/master/source/api/load-balance-openstack-api.html.md"><i class="icon fa fa-pencil"></i>Edit this page on GitHub</a>
</div>
<div class='last-modified'>
Page last modified
Tue 8 Dec 2015 14:43 UTC
</div>
</footer>


<script src="/javascripts/application.js" type="text/javascript"></script>

<!-- begin eloqua tracking -->
<script language='JavaScript' src='//www.redhat.com/j/elqNow/elqCfg.js' type='text/javascript'></script>
<script language='JavaScript' src='//www.redhat.com/j/elqNow/elqImg.js' type='text/javascript'></script>
<!-- end eloqua tracking -->
<!-- begin Omniture tracking -->
<!--
SiteCatalyst code version: H.25.4
Copyright 1996-2013 Adobe, Inc. All Rights Reserved
More info available at http://www.omniture.com
-->
<div id='oTags'>
<script src='//www.redhat.com/assets/js/noncms_s_code.js' type='text/javascript'></script>
<script>
   /* You may give each page an identifying name, server, and channel on the next lines. */
   var coreUrl = encodeURI(document.URL.split("?")[0]);
   var urlSplit = coreUrl.toLowerCase().split(/\//);
   var urlLast = urlSplit[urlSplit.length-1];
   var pageNameString = "";
   var minorSectionIndex = 3;
   if (urlSplit[3] == "promo") {
     minorSectionIndex = 4;
   }
   if (urlLast == "") {
     urlSplit.splice(-1,1);
   }
   if (urlLast.search(/\./) >= 0) {
     if (urlLast == "index.html" || urlLast == "index.php") {
       urlSplit.splice(-1,1);
     } else {
       urlSplit[urlSplit.length-1] = urlLast.split(".").splice(0,1);
     }
   }
   s.prop14 = s.eVar27 = "rdo";
   s.prop15 = s.eVar28 = urlSplit[minorSectionIndex] || "";
   s.prop16 = s.eVar29 = urlSplit[minorSectionIndex+1] || "";
   pageNameString = urlSplit.splice(3).join(" | ");
   s.pageName = "rh | microsite | rdo | " + pageNameString;
   s.channel = "microsite";
   s.prop4 = s.eVar23 = encodeURI(document.URL);
   s.prop21 = s.eVar18 = coreUrl;
   s.prop2 = s.eVar22 = "en";
</script>
<script src='//www.redhat.com/j/rh_omni_footer.js' type='text/javascript'></script>
<script>
   if(navigator.appVersion.indexOf('MSIE')>=0)document.write(unescape('%3C')+'\!-'+'-')
</script>
<noscript>
<a href='http://www.omniture.com' title='Web Analytics'>
<img alt='' border='0' height='1' src='https://smtrcs.redhat.com/b/ss/redhatcom,redhatglobal/1/H.25.4--NS/0?[AQB]=3[AQE]' width='1'>
</a>
</noscript>
</div>
<!-- /DO NOT REMOVE/ -->
<!-- End SiteCatalyst code version: H.25.4 -->
<!-- End Omniture tracking -->

</body>
</html>

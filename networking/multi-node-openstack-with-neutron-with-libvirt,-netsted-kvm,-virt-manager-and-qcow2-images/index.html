<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8 lt-ie7" lang="en-us"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8" lang="en-us"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if IE 9]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if lt IE 10]> <html class="no-js lt-ie10" lang="en-us"> <![endif]-->
<!--[if !IE]> > <![endif]-->
<html class='no-js' lang='en'>
<!-- <![endif] -->
<head>
<title>
Multi-node Openstack with Neutron with libvirt, netsted kvm, virt-manager and qcow2 images &mdash;
RDO
</title>
<meta charset='utf-8'>
<meta content='' name='description'>
<meta content='' name='author'>
<meta content='initial-scale=1.0,user-scalable=no,maximum-scale=1,width=device-width' name='viewport'>

<link href='/images/favicon.ico' rel='shortcut icon'>
<link href='/images/apple-touch-icon-precomposed.png' rel='apple-touch-icon-precomposed'>
<link href='/images/apple-touch-icon-57x57-precomposed.png' rel='apple-touch-icon-precomposed' sizes='57x57'>
<link href='/images/apple-touch-icon-72x72-precomposed.png' rel='apple-touch-icon-precomposed' sizes='72x72'>
<link href='/images/apple-touch-icon-114x114-precomposed.png' rel='apple-touch-icon-precomposed' sizes='114x114'>
<link href="/stylesheets/application.css" rel="stylesheet" type="text/css" />
<link href="/stylesheets/print.css" rel="stylesheet" type="text/css" media="print" />
</head>
<body class=' source-md'>
<header class='masthead hidden-print' id='branding' role='banner'>
<a href='https://github.com/redhat-openstack/website/edit/master/source/networking/multi-node-openstack-with-neutron-with-libvirt,-netsted-kvm,-virt-manager-and-qcow2-images.html.md'>
<img alt='Edit on GitHub' data-canonical-src='/images/edit.png' src='/images/edit.png' style='position: absolute; top: 0; right: 0; border: 0;'>
</a>
<section class='hgroup'>
<h1>
<a href="/"><img id="logo" class="logo " alt="RDO" src="/images/rdo-logo-white.png" />
</a></h1>
</section>
<div id='access'>
<nav role='navigation'>
<ul class='nav nav-pills'>
<li class='nav-link-home' role='menuitem'>
<a href='/'>Home</a>
</li>

<li class='nav-link-blog' role='menuitem'>
<a href='/blog/'>Blog</a>
</li>

<li class='nav-link-events' role='menuitem'>
<a href='/events/'>Events</a>
</li>

<li class='nav-link-community' role='menuitem'>
<a href='/community/'>Community</a>
</li>

<li class='nav-link-docs' role='menuitem'>
<a href='/documentation/'>Docs</a>
</li>

<li class='nav-link-search' role='menuitem'>
<a href='/search/'>Search</a>
</li>

</ul>
</nav>

</div>
</header>

<section class='page-wrap' id='page-wrap'>
<section class='page' id='page'>
<section class='container content' id='content'>

<!--[if lt IE 7]>
<p class="chromeframe">You are using an outdated browser.
<a href="http://browsehappy.com/">Upgrade your browser today</a> or
<a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
<![endif]-->
<h1 id="multi-node-openstack-with-neutron-with-libvirt-netsted-kvm-virt-manager-and-qcow2-images">Multi-node Openstack with Neutron with libvirt, netsted kvm, virt-manager and qcow2 images</h1>

<h2 id="introduction">Introduction</h2>

<p>The goal of this process is to end up with a RHEL-based multi-node environment for working on Openstack with Neutron and packstack on a single physical machine using libvirt, netsted kvm, virt-manager and qcow2 images. This setup uses two libvirt networks, one for external communication and one data network for internal communication between openstack services. Four instances will be created: one controller node, two compute nodes, and a network node.</p>

<h2 id="setting-up-the-physical-machine">Setting up the physical machine</h2>

<p>Start with F19 default Gnome Desktop install, setting up your user account, etc.</p>

<h3 id="install-required-packages">Install required packages</h3>

<pre class="highlight plaintext"><code>$ sudo yum install virt-manager qemu-kvm sys libguestfs-tools&#x000A;</code></pre>

<h3 id="enable-nested-kvm">Enable nested kvm</h3>

<pre class="highlight plaintext"><code>$ echo "options kvm-intel nested=y" | sudo tee /etc/modprobe.d/kvm-intel.conf&#x000A;$ reboot&#x000A;</code></pre>

<h3 id="creating-a-rhel-64-base-install-vm">Creating a RHEL 6.4 base install VM</h3>

<ul>
  <li>Start virt-manager</li>
  <li>Click New VM</li>
</ul>

<p><img alt="" title="new_vm.png" src="/images/wiki/New_vm.png" /></p>

<ul>
  <li>Fields
    <ul>
      <li>Name: rhel-6.4</li>
      <li>Network Install</li>
    </ul>
  </li>
  <li>Click Forward</li>
</ul>

<p><img alt="" title="net_install.png" src="/images/wiki/Net_install.png" /></p>

<ul>
  <li>Fields
    <ul>
      <li>URL: The network install location for your distro, e.g. <a href="http:">http://</a><server>/RHEL-6/6.4/Server/x86_64/os/</server></li>
    </ul>
  </li>
  <li>Click Forward</li>
</ul>

<p><img alt="" title="memory_cpu.png" src="/images/wiki/Memory_cpu.png" /></p>

<ul>
  <li>Fields
    <ul>
      <li>Memory: 2048</li>
      <li>CPU: 2</li>
    </ul>
  </li>
  <li>Click Forward</li>
</ul>

<p><img alt="" title="vm_storage_cfg.png" src="/images/wiki/Vm_storage_cfg.png" /></p>

<ul>
  <li>Fields
    <ul>
      <li>Check Enable storage for this virtual machine</li>
      <li>Select Select managed or other existing storage</li>
    </ul>
  </li>
  <li>Click Browse</li>
</ul>

<p><img alt="" title="manage_storage.png" src="/images/wiki/Manage_storage.png" /></p>

<ul>
  <li>Click New Volume</li>
</ul>

<p>To enable snapshot capability, we need to ensure that VMs are backed by a qcow2 image instead of the default raw.</p>

<p><img alt="" title="new_volume.png" src="/images/wiki/New_volume.png" /></p>

<ul>
  <li>Fields
    <ul>
      <li>Name: rhel-6.4</li>
      <li>Format: qcow2</li>
      <li>Max Capacity: 8192MB (Works for me, but I don't use cinder. Might as well make this 30GB if you want space to store lots of images)</li>
      <li>Allocation: 8192MB (I pre-allocate out of habit)</li>
    </ul>
  </li>
  <li>Click Finish</li>
</ul>

<p>Select rhel-6.4.img and click Choose Volume Click Forward</p>

<p>Check "Customize configuration before install" and click Finish</p>

<p><img alt="" title="8-configure_cpu.png" src="/images/wiki/8-configure_cpu.png" /></p>

<ul>
  <li>Fields
    <ul>
      <li>Select "Processor" and expand &gt; Configuration and click "Copy host CPU configuration"</li>
      <li>Ensure 'vme' and 'vmx' are set to 'require' and click 'Apply' and then "Begin Installation"</li>
    </ul>
  </li>
  <li>Click Apply</li>
</ul>

<h2 id="setting-up-the-base-rhel-64-vm">Setting up the base RHEL 6.4 VM</h2>

<h3 id="installation">Installation</h3>

<p>Go through the RHEL 6.4 installation process</p>

<ul>
  <li>Notes
    <ul>
      <li>I edit the default partitioning layout to reduce the swap size by half and give the extra space to lv_root. If you make your base install larger than I do, you don't have to worry so much about this.</li>
      <li>Select "Minimal" install (not Basic Server)</li>
    </ul>
  </li>
</ul>

<h3 id="configuration">Configuration</h3>

<p>After VM installation completes and reboot, we need to do some basic setup and make a snapshot of the fresh install so you can just clone new VMs from it. Make sure your base distro repositories are set up, RHEL registration is done, etc.</p>

<p>Install the EPEL repo:</p>

<pre class="highlight plaintext"><code># yum -y localinstall http://mirrors.kernel.org/fedora-epel/6/i386/epel-release-6-8.noarch.rpm&#x000A;</code></pre>

<p>and update the system</p>

<pre class="highlight plaintext"><code># yum -y update&#x000A;</code></pre>

<p>To take a snapshot, open up a terminal on your physical machine and run:</p>

<pre class="highlight plaintext"><code>sudo virsh snapshot-create-as rhel-6.4 fresh_install "Fresh RHEL 6.4 install" --atomic --reuse-external&#x000A;</code></pre>

<p>After creating the base install snapshot, set up the repo for the RDO version you are targeting for development.</p>

<p>RDO-grizzly:</p>

<pre class="highlight plaintext"><code># yum install -y http://rdo.fedorapeople.org/openstack-grizzly/rdo-release-grizzly.rpm&#x000A;</code></pre>

<p>RDO-havana:</p>

<pre class="highlight plaintext"><code>yum install -y http://rdo.fedorapeople.org/openstack-havana/rdo-release-havana.rpm&#x000A;</code></pre>

<p>NOTE: Repository mirroring isn't in the scope of this document, but if you are not on the same LAN as your yum repositories, it is an extremely good idea to create a local mirror of your repositories. A quick and dirty method is to essentially copy over all of the /etc/yum.repos.d/ repo contents to something like ~/reposync/yum.conf, then:</p>

<pre class="highlight plaintext"><code>reposync -n -c yum.conf&#x000A;for i in */;do createrepo -c cache $i;done.&#x000A;&#x000A;  and a quick and dirty way to serve them up is:&#x000A;&#x000A;sudo python -m SimpleHTTPServer 80&#x000A;</code></pre>

<p>(make sure to open port 80 on your firewall).</p>

<p>At this point, the rhel-6.4 VM should be powered off.</p>

<h2 id="setting-up-the-openstack-nodes">Setting up the Openstack nodes</h2>

<h3 id="introduction-1">Introduction</h3>

<p>We are going to create 4 VMs:</p>

<p>1x Controller node: All of the openstack API services go here
2x Compute node: VMs will be launched from here
1x Network node: Neutron L3 and DCHP services will be run from here.</p>

<p>This installation will use two different libvirt networks:</p>

<p>data: This will be the "internal" network that openstack services use to communicate with each other. We will have to create this network.
default: This is the default libvirt network. This will serve as our "external" network. It is a NATed connection and will have access to the outside world. Theoretically, only the Network Node will need this connection–but this is only the case if we have local mirrors set up accessible via the 'data' network for all of the repositories we need.</p>

<p>This document will assume that a local mirror has *not* exist and add the default network to each VM. If you have a local mirror, just add the default network interface to the Network Node.</p>

<h3 id="creating-the-data-network">Creating the "data" network</h3>

<p>Edit -&gt; Connection Details -&gt; Virtual Networks</p>

<p><img alt="" title="9-new_virtual_network.png" src="/images/wiki/9-new_virtual_network.png" /></p>

<ul>
  <li>Click '+' and the Click Forward through the instructions</li>
</ul>

<p><img alt="" title="10-name_virtual_netowrk.png" src="/images/wiki/10-name_virtual_netowrk.png" /></p>

<ul>
  <li>Fields
    <ul>
      <li>Network Name: data</li>
    </ul>
  </li>
  <li>Click Forward</li>
</ul>

<p><img alt="" title="11-ipv4_network_setup.png" src="/images/wiki/11-ipv4_network_setup.png" /></p>

<ul>
  <li>Fields
    <ul>
      <li>Check Enable IPv4 network address space definition</li>
      <li>Network:192.168.100.0/24</li>
      <li>Enable DHCPv4 (we will add static DHCP assignments for our instances later)</li>
    </ul>
  </li>
  <li>Click Forward</li>
</ul>

<!-- -->

<ul>
  <li>Skip IPv6 for now (sigh)</li>
  <li>Click Forward</li>
</ul>

<p><img alt="" title="12-misc_network_setup.png" src="/images/wiki/12-misc_network_setup.png" /></p>

<ul>
  <li>Fields
    <ul>
      <li>Select isolated virtual network</li>
    </ul>
  </li>
  <li>Click Forward</li>
</ul>

<p><img alt="" title="13-finalize_network_setup.png" src="/images/wiki/13-finalize_network_setup.png" /></p>

<ul>
  <li>Click Finish</li>
</ul>

<h3 id="cloning-the-openstack-service-nodes">Cloning the openstack service nodes</h3>

<p>First, clone rhel-6.4 for the controller node: Since we have an snapshot of the existing install already, we can run virt-sysprep to fix some of the issues that cloning would cause. First and foremost is that /etc/udev/rules.d/70-persistent-net.rules and /etc/sysconfig/network-scripts/ifcfg-eth0 will have the hard-coded MACs from the cloned VM which will change.</p>

<p>From a terminal on the physical machine:</p>

<pre class="highlight plaintext"><code>$ sudo virt-sysprep -d rhel-6.4&#x000A;</code></pre>

<p>From virt-manager: Right-click rhel-6.4 -&gt; Clone</p>

<p><img alt="" title="14-clone_controller.png" src="/images/wiki/14-clone_controller.png" /></p>

<ul>
  <li>Fields
    <ul>
      <li>Name: rhel-6.4-controller</li>
      <li>Networking: NAT</li>
      <li>Storage: rhel-6.4.img
        <ul>
          <li>Clone this disk (8.0 GB)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Click Clone</li>
</ul>

<p>Select rhel-6.4-controller and click Open View -&gt; Details</p>

<p><img alt="" title="fig:15-add_hardware.png" src="/images/wiki/15-add_hardware.png" /> Click Add Hardware Select Network</p>

<p><img alt="" title="16-add_network_interface.png" src="/images/wiki/16-add_network_interface.png" /></p>

<ul>
  <li>Fields
    <ul>
      <li>Host device: Virtual network 'data': Isolated network, internal and host routing only</li>
    </ul>
  </li>
  <li>Click Finish</li>
</ul>

<p>NOTE: If you have a local mirror of your yum repositories, instead of adding a new network interface, modify the existing network interface to Source device: Virtual network 'data': Isolated network, internal and host routing only</p>

<h3 id="add-static-dhcp-address-mapping">Add static DHCP address mapping</h3>

<p>It's ugly, but you can add that static DHCP entry for the VM by:</p>

<pre class="highlight plaintext"><code>sudo virsh net-update data add-last ip-dhcp-host --xml "&lt;host mac='`sudo virsh domiflist rhel-6.4-controller|grep data|awk '{print $5}'`' ip='192.168.100.10'/&gt;" --live --config&#x000A;</code></pre>

<p>Since parsing human-readable output from CLI commands that are just wrappers for an API is evil, I also wrote <a href="https://github.com/otherwiseguy/virt-add-static-dhcp">https://github.com/otherwiseguy/virt-add-static-dhcp</a> to quickly do this part using the libvirt python bindings.</p>

<h3 id="add-the-eth1-network-script">Add the eth1 network script</h3>

<p>Add a second network adapter using Virtual Machine Manager for the script to bind to.</p>

<p>View -&gt; Console Virtual Machine -&gt; Run</p>

<p>Add a startup script for the "data" interface:</p>

<pre class="highlight plaintext"><code>cp /etc/sysconfig/network-scripts/ifcfg-eth{0,1}&#x000A;</code></pre>

<p>Edit /etc/sysconfig/network-scripts/ifcfg-eth1: DEVICE=eth1, delete UUID line, and add PEERDNS=no</p>

<pre class="highlight plaintext"><code>service network restart&#x000A;</code></pre>

<p>Ensure that eth0 and eth1 have addresses and that eth1's address is the static IP we assigned above</p>

<p>At this point, it is also usually a good idea to go ahead and update the packages on the system, since we will be cloning this vm for our other nodes.</p>

<pre class="highlight plaintext"><code>yum -y update&#x000A;</code></pre>

<h3 id="create-a-snapshot-for-the-base-install-of-the-new-vm">Create a snapshot for the base install of the new VM</h3>

<p>Make a snapshot for rhel-6.4-controller</p>

<pre class="highlight plaintext"><code>sudo virsh snapshot-create-as rhel-6.4-controller base "RHEL 6.4 controller base" --atomic --reuse-external&#x000A;</code></pre>

<p>Power off rhel-6.4-controller</p>

<p>Repeat, cloning rhel-6.4-compute1 (192.168.100.20), rhel-6.4-compute2 (192.168.100.21), rhel-6.4-network (192.168.100.30) VMs from rhel-6.4-controller, running:</p>

<pre class="highlight plaintext"><code>$ sudo virt-sysprep -d rhel-6.4-controller&#x000A;</code></pre>

<p>before each clone.</p>

<h3 id="reverting-to-a-snapshot">Reverting to a snapshot</h3>

<p>For example, to revert the controller to the base snapshot:</p>

<pre class="highlight plaintext"><code>sudo virsh snapshot-revert --force rhel-6.4-controller base&#x000A;</code></pre>

<category:networking>
</category:networking>

</section>
</section>
</section>
<footer class='text-center' id='footer'>
<hr class='visible-print'>
<div class='footer-rdo'>
<dl>
  <dt>Docs</dt>
  <dd><a href="/install/quickstart">Download</a></dd>
  <dd><a href="/rdo/faq">Common questions</a></dd>
  <dd><a href="/troubleshooting/">Troubleshooting</a></dd>
  <dd><a href="/rdo/">About RDO</a></dd>
</dl>

<dl>
  <dt>Use RDO</dt>
  <dd><a href="/install/quickstart">Quickstart</a></dd>
  <dd><a href="/rdo/release-cadence">Releases</a></dd>
  <dd><a href="http://trunk.rdoproject.org/">Trunk builds</a></dd>
</dl>

<dl>
  <dt>Community</dt>
  <dd><a href="/community/">Participate</a></dd>
  <dd><a href="http://ask.rdoproject.org/">Ask (Q&amp;A)</a></dd>
  <dd><a href="https://bugzilla.redhat.com/buglist.cgi?product=RDO&amp;query_format=advanced&amp;bug_status=NEW&amp;bug_status=ASSIGNED">Browse open issues</a></dd>
  <dd><a href="https://bugzilla.redhat.com/enter_bug.cgi?product=RDO">Report a problem</a></dd>
</dl>

<dl>
  <dt>Support</dt>
  <dd><a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a></dd>
  <dd><a href="https://www.redhat.com/mailman/listinfo/rdo-list">Contact us</a></dd>
</dl>

</div>

<ul class='footer-nav-list'>
<li><strong>RDO</strong> is a <strong>Red Hat</strong>-sponsored community project</li>
</ul>

<div class='copyright'>
&copy; 2016 RDO
<span class='legal'><a href="/legal/">Legal & Privacy</a></span>
</div>
<div class='edit-this-page'>
<a target="_blank" href="https://github.com/redhat-openstack/website/edit/master/source/networking/multi-node-openstack-with-neutron-with-libvirt,-netsted-kvm,-virt-manager-and-qcow2-images.html.md"><i class="icon fa fa-pencil"></i>Edit this page on GitHub</a>
</div>
<div class='last-modified'>
Page last modified
Wed 20 Jan 2016 16:28 UTC
</div>
</footer>


<script src="/javascripts/application.js" type="text/javascript"></script>

<!-- begin eloqua tracking -->
<script language='JavaScript' src='//www.redhat.com/j/elqNow/elqCfg.js' type='text/javascript'></script>
<script language='JavaScript' src='//www.redhat.com/j/elqNow/elqImg.js' type='text/javascript'></script>
<!-- end eloqua tracking -->
<!-- begin Omniture tracking -->
<!--
SiteCatalyst code version: H.25.4
Copyright 1996-2013 Adobe, Inc. All Rights Reserved
More info available at http://www.omniture.com
-->
<div id='oTags'>
<script src='//www.redhat.com/assets/js/noncms_s_code.js' type='text/javascript'></script>
<script>
   /* You may give each page an identifying name, server, and channel on the next lines. */
   var coreUrl = encodeURI(document.URL.split("?")[0]);
   var urlSplit = coreUrl.toLowerCase().split(/\//);
   var urlLast = urlSplit[urlSplit.length-1];
   var pageNameString = "";
   var minorSectionIndex = 3;
   if (urlSplit[3] == "promo") {
     minorSectionIndex = 4;
   }
   if (urlLast == "") {
     urlSplit.splice(-1,1);
   }
   if (urlLast.search(/\./) >= 0) {
     if (urlLast == "index.html" || urlLast == "index.php") {
       urlSplit.splice(-1,1);
     } else {
       urlSplit[urlSplit.length-1] = urlLast.split(".").splice(0,1);
     }
   }
   s.prop14 = s.eVar27 = "rdo";
   s.prop15 = s.eVar28 = urlSplit[minorSectionIndex] || "";
   s.prop16 = s.eVar29 = urlSplit[minorSectionIndex+1] || "";
   pageNameString = urlSplit.splice(3).join(" | ");
   s.pageName = "rh | microsite | rdo | " + pageNameString;
   s.channel = "microsite";
   s.prop4 = s.eVar23 = encodeURI(document.URL);
   s.prop21 = s.eVar18 = coreUrl;
   s.prop2 = s.eVar22 = "en";
</script>
<script src='//www.redhat.com/j/rh_omni_footer.js' type='text/javascript'></script>
<script>
   if(navigator.appVersion.indexOf('MSIE')>=0)document.write(unescape('%3C')+'\!-'+'-')
</script>
<noscript>
<a href='http://www.omniture.com' title='Web Analytics'>
<img alt='' border='0' height='1' src='https://smtrcs.redhat.com/b/ss/redhatcom,redhatglobal/1/H.25.4--NS/0?[AQB]=3[AQE]' width='1'>
</a>
</noscript>
</div>
<!-- /DO NOT REMOVE/ -->
<!-- End SiteCatalyst code version: H.25.4 -->
<!-- End Omniture tracking -->

</body>
</html>

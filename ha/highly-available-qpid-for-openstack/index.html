<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8 lt-ie7" lang="en-us"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie10 lt-ie9 lt-ie8" lang="en-us"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if IE 9]> <html class="no-js lt-ie10 lt-ie9" lang="en-us"> <![endif]-->
<!--[if lt IE 10]> <html class="no-js lt-ie10" lang="en-us"> <![endif]-->
<!--[if !IE]> > <![endif]-->
<html class='no-js' lang='en'>
<!-- <![endif] -->
<head>
<title>
Highly Available Qpid for OpenStack &mdash;
RDO
</title>
<meta charset='utf-8'>
<meta content='' name='description'>
<meta content='' name='author'>
<meta content='initial-scale=1.0,user-scalable=no,maximum-scale=1,width=device-width' name='viewport'>

<link href='/images/favicon.ico' rel='shortcut icon'>
<link href='/images/apple-touch-icon-precomposed.png' rel='apple-touch-icon-precomposed'>
<link href='/images/apple-touch-icon-57x57-precomposed.png' rel='apple-touch-icon-precomposed' sizes='57x57'>
<link href='/images/apple-touch-icon-72x72-precomposed.png' rel='apple-touch-icon-precomposed' sizes='72x72'>
<link href='/images/apple-touch-icon-114x114-precomposed.png' rel='apple-touch-icon-precomposed' sizes='114x114'>
<link href="/stylesheets/application.css" rel="stylesheet" type="text/css" />
<link href="/stylesheets/print.css" rel="stylesheet" type="text/css" media="print" />
</head>
<body class=' source-md'>
<header class='masthead hidden-print' id='branding' role='banner'>
<a href='https://github.com/redhat-openstack/website/edit/master/source/ha/highly-available-qpid-for-openstack.html.md'>
<img alt='Edit on GitHub' data-canonical-src='/images/edit.png' src='/images/edit.png' style='position: absolute; top: 0; right: 0; border: 0;'>
</a>
<section class='hgroup'>
<h1>
<a href="/"><img id="logo" class="logo " alt="RDO" src="/images/rdo-logo-white.png" />
</a></h1>
</section>
<div id='access'>
<nav role='navigation'>
<ul class='nav nav-pills'>
<li class='nav-link-home' role='menuitem'>
<a href='/'>Home</a>
</li>

<li class='nav-link-blog' role='menuitem'>
<a href='/blog/'>Blog</a>
</li>

<li class='nav-link-events' role='menuitem'>
<a href='/events/'>Events</a>
</li>

<li class='nav-link-community' role='menuitem'>
<a href='/community/'>Community</a>
</li>

<li class='nav-link-docs' role='menuitem'>
<a href='/documentation/'>Docs</a>
</li>

<li class='nav-link-search' role='menuitem'>
<a href='/search/'>Search</a>
</li>

</ul>
</nav>

</div>
</header>

<section class='page-wrap' id='page-wrap'>
<section class='page' id='page'>
<section class='container content' id='content'>

<!--[if lt IE 7]>
<p class="chromeframe">You are using an outdated browser.
<a href="http://browsehappy.com/">Upgrade your browser today</a> or
<a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
<![endif]-->
<h1 id="highly-available-qpid-for-openstack">Highly Available Qpid for OpenStack</h1>

<h2 id="implementation-options">Implementation Options</h2>

<p>This document will cover 3 possible configurations available for Qpid to be clustered and/or Highly Available. Each are described here, with configuration later in the document.</p>

<p>IMPORTANT: OpenStack is built to resend messages, clustering is not necessary. In OpenStack HA means Highly Available services and does not require guaranteed message delivery or message persistence. The first solution meets the requirements for HA OpenStack messaging. The second and third solutions are valid HA messaging configurations for OpenStack but are not necessary for a safe OpenStack HA messaging deployment. These latter two options come with substantial additional overhead.</p>

<ol>
  <li>Pacemaker managed without Clustering</li>
</ol>

<p>This configuration will make the Qpid broker highly available but does not prevent message loss within the messaging brokers. However, as mentioned above, OpenStack is built to resend messages, message loss is NOT an issue and OpenStack will recover. With this implementation Qpid will run on only one of the pacemaker nodes at a time, grouped with a floating IP address. In the event of a node failure pacemaker will start the Qpid broker and floating IP on another node. All OpenStack components will point to the floating IP grouped with the Qpid broker for messaging services. Service will be resumed.</p>

<ol>
  <li>Clustered without pacemaker</li>
</ol>

<p>This configuration runs multiple Qpid brokers in a clustered configuration. This prevents message loss, though will require some other form of monitoring for the Qpid brokers.</p>

<ol>
  <li>Pacemaker managed with clustering</li>
</ol>

<p>This configuration makes the Qpid broker highly available and prevents message loss. Qpid runs with multiple brokers and is monitored by pacemaker.</p>

<p>WARNING: Only set up one of these solutions. These solutions are not intended to be run simultaneously.</p>

<h2 id="pacemaker-managed-without-clustering">Pacemaker managed without clustering</h2>

<p>All that needs to be done is to install qpid on the pacemaker nodes and add it as a pacemaker resource. You can give qpid its own floating ip or just add it in the same group that already has a floating ip. For this demonstration they will be added to a group that already has an existing ip and MySQL.</p>

<pre class="highlight plaintext"><code>$ pcs resource create qpidd lsb:qpidd --group test-group&#x000A;</code></pre>

<p>Finally ensure that all the qpid settings in the openstack components point to the floating ip of the group you added qpid to.</p>

<pre class="highlight plaintext"><code># Qpid broker hostname (string value)&#x000A;qpid_hostname=192.168.122.203&#x000A;# Qpid broker port (integer value)&#x000A;qpid_port=5672&#x000A;</code></pre>

<p>WARNING: This method of managing qpid through pacemaker makes the service highly available but does not prevent message loss.</p>

<h2 id="clustered-without-pacemaker">Clustered without pacemaker</h2>

<p>The previous example of setting qpid up highly available lets it co-exist with pacemaker but doesn't prevent message loss. If you setup a separate set of nodes to run a qpid cluster then message loss can be prevented. In this scenario qpid must run on a separate set of nodes from pacemaker.</p>

<p>To do this, do the following on each qpid cluster node. First install the qpid cluster packages.</p>

<pre class="highlight plaintext"><code># yum install -y qpid-cpp-server-cluster qpid-tools&#x000A;</code></pre>

<p>Ensure that the corosync uidgid.d directory has a qpidd file with the following contents:</p>

<pre class="highlight plaintext"><code># cat /etc/corosync/uidgid.d/qpidd &#x000A;uidgid {&#x000A;        uid: qpidd&#x000A;        gid: qpidd&#x000A;}&#x000A;</code></pre>

<p>copy the corosync.conf sample file into place</p>

<pre class="highlight plaintext"><code># cp /etc/corosync/corosync.conf.sample /etc/corosync/corosync.conf&#x000A;</code></pre>

<p>Set the corosync.conf bindnetaddr property to the network address for the interface.
This can be calculated by doing a bitwise AND of the inet addr. For example a 172.16.44.123/255.255.248.0 would result in bindnetaddr being set to 172.16.40.0. In this example 192.168.122.101/255.255.255.0 and 192.168.122.102/255.255.255.0 result in the bindnetaddr value of 192.168.122.0 for both nodes in the qpid cluster.</p>

<pre class="highlight plaintext"><code>        bindnetaddr: 192.168.122.0&#x000A;</code></pre>

<p>Open up a bunch of ports for the cluster to talk</p>

<pre class="highlight plaintext"><code>iptables -I INPUT -p udp -m udp --dport 5405  -j ACCEPT&#x000A;iptables -I INPUT -p tcp -m tcp --dport 5405  -j ACCEPT&#x000A;iptables -I INPUT -p tcp -m tcp --dport 8084  -j ACCEPT&#x000A;iptables -I INPUT -p tcp -m tcp --dport 11111  -j ACCEPT&#x000A;iptables -I INPUT -p tcp -m tcp --dport 14567  -j ACCEPT&#x000A;iptables -I INPUT -p tcp -m tcp --dport 16851  -j ACCEPT&#x000A;iptables -I INPUT -p tcp -m tcp --dport 21064  -j ACCEPT&#x000A;iptables -I INPUT -p tcp -m tcp --dport 50006  -j ACCEPT&#x000A;iptables -I INPUT -p udp -m udp --dport 50007  -j ACCEPT&#x000A;iptables -I INPUT -p tcp -m tcp --dport 50008  -j ACCEPT&#x000A;iptables -I INPUT -p tcp -m tcp --dport 50009  -j ACCEPT&#x000A;service iptables save&#x000A;service iptables restart&#x000A;</code></pre>

<p>start corosync and then qpid</p>

<pre class="highlight plaintext"><code># service corosync start&#x000A;# service qpidd start&#x000A;</code></pre>

<p>Finally replace all the qpid settings in the openstack components to use the qpid_hosts parameter instead of the qpid_hostname and qpid_port parameters</p>

<pre class="highlight plaintext"><code># Qpid broker hostname (string value)&#x000A;#qpid_hostname=localhost&#x000A;# Qpid broker port (integer value)&#x000A;#qpid_port=5672&#x000A;# Qpid HA cluster host:port pairs (list value)&#x000A;qpid_hosts=192.168.122.101:5672,192.168.122.102:5672&#x000A;</code></pre>

<p>WARNING: This configuration requires another form of monitoring the qpid brokers.</p>

<h2 id="pacemaker-managed-with-clustering">Pacemaker managed with clustering</h2>

<p>The previous example of setting qpid up highly available lets it co-exist with pacemaker but doesn't prevent message loss. If you setup a separate set of nodes to run a qpid cluster then message loss can be prevented but then pacemaker isn't managing the service. In this scenario qpid runs on the pacemaker nodes and is configured for clustering to prevent message loss.</p>

<p>To do this, start with a working pacemaker install. Than do the following on each pacemaker/qpid cluster node. First install the qpid cluster packages.</p>

<pre class="highlight plaintext"><code># yum install -y qpid-cpp-server-cluster qpid-tools&#x000A;</code></pre>

<p>Add a uidgid entry to the /etc/cluster/cluster.conf file:</p>

<pre class="highlight plaintext"><code># cat /etc/cluster/cluster.conf &#x000A;&lt;cluster config_version="9" cluster_name="cluster_name"&gt;&#x000A;        &lt;uidgid uid="qpidd" gid="qpidd" /&gt;&#x000A;       ... other content ...&#x000A;&lt;/cluster&gt;&#x000A;</code></pre>

<p>On only one node add pacemaker as a cloned resource to pacemaker. A cloned resource runs on all pacemaker nodes. A floating ip is not needed in this configuration because the clients will know about all the brokers.</p>

<pre class="highlight plaintext"><code>node1$ pcs resource create qpidd lsb:qpidd --clone&#x000A;</code></pre>

<p>Finally replace all the qpid settings in the openstack components to use the qpid_hosts parameter instead of the qpid_hostname and qpid_port parameters</p>

<pre class="highlight plaintext"><code># Qpid broker hostname (string value)&#x000A;#qpid_hostname=localhost&#x000A;# Qpid broker port (integer value)&#x000A;#qpid_port=5672&#x000A;# Qpid HA cluster host:port pairs (list value)&#x000A;qpid_hosts=192.168.122.101:5672,192.168.122.102:5672&#x000A;</code></pre>

</section>
</section>
</section>
<footer class='text-center' id='footer'>
<hr class='visible-print'>
<div class='footer-rdo'>
<dl>
  <dt>Docs</dt>
  <dd><a href="/install/quickstart">Download</a></dd>
  <dd><a href="/rdo/faq">Common questions</a></dd>
  <dd><a href="/troubleshooting/">Troubleshooting</a></dd>
  <dd><a href="/rdo/">About RDO</a></dd>
</dl>

<dl>
  <dt>Use RDO</dt>
  <dd><a href="/install/quickstart">Quickstart</a></dd>
  <dd><a href="/rdo/release-cadence">Releases</a></dd>
  <dd><a href="http://trunk.rdoproject.org/">Trunk builds</a></dd>
</dl>

<dl>
  <dt>Community</dt>
  <dd><a href="/community/">Participate</a></dd>
  <dd><a href="http://ask.rdoproject.org/">Ask (Q&amp;A)</a></dd>
  <dd><a href="https://bugzilla.redhat.com/buglist.cgi?product=RDO&amp;query_format=advanced&amp;bug_status=NEW&amp;bug_status=ASSIGNED">Browse open issues</a></dd>
  <dd><a href="https://bugzilla.redhat.com/enter_bug.cgi?product=RDO">Report a problem</a></dd>
</dl>

<dl>
  <dt>Support</dt>
  <dd><a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a></dd>
  <dd><a href="https://www.redhat.com/mailman/listinfo/rdo-list">Contact us</a></dd>
</dl>

</div>

<ul class='footer-nav-list'>
<li><strong>RDO</strong> is a <strong>Red Hat</strong>-sponsored community project</li>
</ul>

<div class='copyright'>
&copy; 2016 RDO
<span class='legal'><a href="/legal/">Legal & Privacy</a></span>
</div>
<div class='edit-this-page'>
<a target="_blank" href="https://github.com/redhat-openstack/website/edit/master/source/ha/highly-available-qpid-for-openstack.html.md"><i class="icon fa fa-pencil"></i>Edit this page on GitHub</a>
</div>
<div class='last-modified'>
Page last modified
Thu 24 Sep 2015 19:34 UTC
</div>
</footer>


<script src="/javascripts/application.js" type="text/javascript"></script>

<!-- begin eloqua tracking -->
<script language='JavaScript' src='//www.redhat.com/j/elqNow/elqCfg.js' type='text/javascript'></script>
<script language='JavaScript' src='//www.redhat.com/j/elqNow/elqImg.js' type='text/javascript'></script>
<!-- end eloqua tracking -->
<!-- begin Omniture tracking -->
<!--
SiteCatalyst code version: H.25.4
Copyright 1996-2013 Adobe, Inc. All Rights Reserved
More info available at http://www.omniture.com
-->
<div id='oTags'>
<script src='//www.redhat.com/assets/js/noncms_s_code.js' type='text/javascript'></script>
<script>
   /* You may give each page an identifying name, server, and channel on the next lines. */
   var coreUrl = encodeURI(document.URL.split("?")[0]);
   var urlSplit = coreUrl.toLowerCase().split(/\//);
   var urlLast = urlSplit[urlSplit.length-1];
   var pageNameString = "";
   var minorSectionIndex = 3;
   if (urlSplit[3] == "promo") {
     minorSectionIndex = 4;
   }
   if (urlLast == "") {
     urlSplit.splice(-1,1);
   }
   if (urlLast.search(/\./) >= 0) {
     if (urlLast == "index.html" || urlLast == "index.php") {
       urlSplit.splice(-1,1);
     } else {
       urlSplit[urlSplit.length-1] = urlLast.split(".").splice(0,1);
     }
   }
   s.prop14 = s.eVar27 = "rdo";
   s.prop15 = s.eVar28 = urlSplit[minorSectionIndex] || "";
   s.prop16 = s.eVar29 = urlSplit[minorSectionIndex+1] || "";
   pageNameString = urlSplit.splice(3).join(" | ");
   s.pageName = "rh | microsite | rdo | " + pageNameString;
   s.channel = "microsite";
   s.prop4 = s.eVar23 = encodeURI(document.URL);
   s.prop21 = s.eVar18 = coreUrl;
   s.prop2 = s.eVar22 = "en";
</script>
<script src='//www.redhat.com/j/rh_omni_footer.js' type='text/javascript'></script>
<script>
   if(navigator.appVersion.indexOf('MSIE')>=0)document.write(unescape('%3C')+'\!-'+'-')
</script>
<noscript>
<a href='http://www.omniture.com' title='Web Analytics'>
<img alt='' border='0' height='1' src='https://smtrcs.redhat.com/b/ss/redhatcom,redhatglobal/1/H.25.4--NS/0?[AQB]=3[AQE]' width='1'>
</a>
</noscript>
</div>
<!-- /DO NOT REMOVE/ -->
<!-- End SiteCatalyst code version: H.25.4 -->
<!-- End Omniture tracking -->

</body>
</html>

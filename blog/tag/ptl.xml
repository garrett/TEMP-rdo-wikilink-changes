<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RDO</title>
  <subtitle>Tag: Ptl</subtitle>
  <id>http://rdoproject.org/blog/</id>
  <link href="http://rdoproject.org/blog/"/>
  <link href="http://rdoproject.org/blog/tag/ptl.xml" rel="self"/>
  <updated>2016-06-08T17:52:06+02:00</updated>
  <author>
    <name/>
  </author>
  <entry>
    <title>Mike Perez: Cinder in Liberty</title>
    <link rel="alternate" href="http://rdoproject.org/blog/2015/11/mike-perez-cinder-in-liberty/"/>
    <id>http://rdoproject.org/blog/2015/11/mike-perez-cinder-in-liberty/</id>
    <published>2015-11-06T19:59:15+01:00</published>
    <updated>2016-06-09T17:28:37+02:00</updated>
    <author>
      <name>Rich Bowen</name>
    </author>
    <content type="html">&lt;p&gt;Before OpenStack Summit, I interviewed Mike Perez about 
what's new in Cinder in the LIberty release, and what's coming in Mitaka. Unfortunately, 
life got a little busy and I didn't get it posted before Summit. However, with Liberty still 
fresh, this is still very timely content.&lt;/p&gt;

&lt;p&gt;In this interview, Mike talks about the awesome new features that have gone into &lt;a href="https://wiki.openstack.org/wiki/Cinder"&gt;Cinder&lt;/a&gt; for Liberty, and what we can expect to see in April.&lt;/p&gt;

&lt;p&gt;If the audio player below doesn't work for you, you can download the full audio &lt;a href="http://drbacchus.com/podcasts/openstack/mike_perez_cinder.mp3"&gt;HERE&lt;/a&gt;. See also the complete transcript below.&lt;/p&gt;

&lt;audio controls=""&gt;
  &lt;source src="http://drbacchus.com/podcasts/openstack/mike_perez_cinder.mp3" type="audio/mpeg" /&gt;
&lt;/audio&gt;

&lt;p&gt;&lt;strong&gt;Rich&lt;/strong&gt;: Today I'm speaking with Mike Perez, who is the PTL of the Cinder
project. Cinder has been around for â€¦&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mike&lt;/strong&gt;: Since the Folsom release.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: So, quite a while. Thanks for taking time to talk with me.&lt;/p&gt;

&lt;p&gt;I wonder if you could start by giving us a real quick definition of
Cinder. I know that people who are new to OpenStack are frequently a
little bit confused as to what it is, and how it differs from Swift
and various other storage type things that are part of OpenStack.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;M&lt;/strong&gt;: Cinder, just like a variety of other OpenStack projects, is just an
API layer. So unlike other projects like &lt;a href="https://wiki.openstack.org/wiki/Swift"&gt;Swift&lt;/a&gt;, for example, which is
an actual implementation of a type of storage, that's where there
would be a little bit of a difference right there.&lt;/p&gt;

&lt;p&gt;In particular, Cinder is more interested in providing block storage,
as opposed to object storage. They have differences in terms of how
exactly you interact with retrieving the data, as well as adding data
to it, and then they also support different types of use cases. In
particular, just without Cinder or Swift in the picture, if you just
have your OpenStack cloud with Nova and Neutron, you just have
ephemeral storage that you're using with your instances. Typically,
with those type of setups, your storage goes away as soon as the
instance goes away. Adding Cinder into the picture, though, you are
given the ability to have persistent storage, so that the storage
itself becomes completely independent of the actual virtual machine
instance. So when you terminate your virtual machine instance on the
Nova side, there will be a detach of that block storage volume, and
then you can later on attach it to another virtual machine instance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: Tell us about Liberty. What's new in Cinder?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;M&lt;/strong&gt;: Inside of the Liberty release we have 16 new volume drivers.
They're all backed with Cinder continuous integration testing that we
adopt from Tempest. Cinder has one of the most - I would say - the
most drivers out of the OpenStack projects. We're just shy of around
60 volume drivers now. Some of them are from multiple vendors. As I
mentioned before, they are all backed behind the continuous
integration, so we know for a fact, as we are going through a
development cycle in Liberty, these volume drivers, the actual
back-ends themselves, are being tested. Every single vendor is
expected to have a CI system in their own lab, hooked up to a real
storage solution that they are trying to support inside of Cinder.
That patch will bring us - they're expected for their CI to bring up a
CI instance that connects to that storage back-end and runs their
Tempest tests, and verify that that patch doesn't break their
integration with their volume driver. That came back in the Kilo
release. The story's in a lot of different places. It was a lot of fun
getting us to that point. It's been nice talking to other projects,
because I know a lot of other projects are interested in going down
the same path, and it's been great having discussions with a variety
of other PTLs on it.&lt;/p&gt;

&lt;p&gt;And along with that we added in support for image caching. You have an
image inside of Glance that you want to boot up a VM with that image.
And with Cinder in particular, since the storage itself is in some
other remote location from that virtual machine, because we're
attaching over something like ISCSI or fibrechannel, grabbing the
image from wherever Glance is at, whatever Glance store is set up with
Glance itself - could be inside of Swift for example, you have an
image inside of Swift - and you want to put that image onto a newly
allocated space volume inside of Cinder, in order to do that, it has
to go across the network to whatever storage solution Cinder is set up
with, and that can be really time consuming, especially for some of
our users that are using 20-40Gb images. It can take a lot of time to
boot up that VM. Cinder itself has a very generic image caching
solution that we added in for the Liberty release, that allows for the
most popular images inside of your cloud to be cached within the
back-end storage solution. So what that means is, for that cache we
can do some really smart things with the different storage solution.
Instead of doing copy of the actual image, we can do copy-on-writes,
for example. So it's pretty much a zero copy, you're just pointing a
reference to that image that's already allocated on that storage
solution. And then create a new volume off of that reference pointer.
Bam! You have a new volume that is set up with that image, and there's
zero copies that had to happen. I think that's going to take care of a
really popular problem, that comes up, and should make users really
happy for this release.&lt;/p&gt;

&lt;p&gt;Some of the other things that we added support for that are kind of
general things is nested quota support, for example. Keystone has this
ability with having projects within projects. Users and operators want
to be able to have the ability to set quotas based on having a
hierarchy for quotas. I'm not sure about other projects, but Cinder
has added this in now for the Liberty release, so you will be able to
add that with your different volume allocation, as well as your
gigabyte allocation as well, for project hierarchy.&lt;/p&gt;

&lt;p&gt;The third piece I would add in is the non-disruptive backups. Cinder
has this ability for doing backups. These are different from
snapshots. Snapshots, you're typically doing the actual volume itself.
You're copying the entire allocation of that volume. So, let's say you
have a 100G volume, and you're only using 5G of it, it's still doing a
snapshot of 100G, even unused parts. The difference with a backup is
you have the ability to do a backup of just the contents, what you're
actually using. And on top of it you're also able to have the volume
backed up to another location, let's say you could set up a Cinder
backup service that is able to talk to Swift. For your Cinder side,
you have Ceph set up. So you have completely two separate storage
solutions, and if something were to happen to the Ceph storage
solution, everything would still be backed up on this other Swift
cluster that's completely independent.&lt;/p&gt;

&lt;p&gt;The problem with doing backups, though, typically was - and this is
just with block storage in general - is you would have to detach the
volume from it being used by the VM, and then you would have to
initiate a backup, and then reattach that volume to the Cinder backup
solution which would then go ahead and put it into the Swift cluster.
That could be disruptive. The new solution that now we have is, some
of the different back-end solutions that we're using for the Cinder
backups will give you the ability to keep the volume attached, and
actively being used, but we can go ahead and initiate a backup, and it
will keep copying over the bits, and hopefully eventually catch up to
where the volume is at, to a point where it can stop the backups, and
your volume can continue being used in production.&lt;/p&gt;

&lt;p&gt;I would say those are the most interesting for the Liberty release.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: What's your vision, looking forward, for Mitaka and on, for Cinder.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;M&lt;/strong&gt;: I sort of talked about this in the previous Liberty release, and
unfortunately we weren't exactly able to finish everything up in time.
My apologies to everyone. But for the operator side, we have been
working on trying to make rolling upgrades a real thing, instead of
just talking about it. Initially, for the Kilo release, we worked on a
solution inside of Oslo that gives you the ability to have the
OpenStack services themselves be written in a way to be independent
from the actual database solution that's being used. Typically when
you do upgrade you have to bring down - whatever services you're going
to upgrade, you have bring down those services, and then you have to
do a database upgrade. And for real OpenStack users, that can be very
time consuming because of all the different column changes that have
to happen for all of the different rows of data that they have.
During that time, you have down time of those services. So, for the
Kilo release we ended up working on a solution that made those
services where they could continue running, and you could run the
database upgrade and there would be no disruption. And when you found
it convenient to do it, you can restart those services so that they
could take advantage of the new database changes. But they could
continue to run even with the database changes happening underneath.
So that was added in Oslo, and we made that available for all
OpenStack projects, not just Cinder, to take advantage of.&lt;/p&gt;

&lt;p&gt;What we're working on for the M release, and what we were working on for Liberty
as well, is the ability to have the services be independent of each
other. The way that they communicate with each other is with RPC. For
example, if you have, in the Cinder case, typical setup is you have a
Cinder API server, you have a Cinder scheduler, and you have a Cinder
volume manager. And as soon as you upgrade one of those services, they
can no longer talk to each other. So, having the ability, as an
operator, to roll out the updates to, say, all of my Cinder APIs, and
verify that everything looks good, and then to make the decision, ok,
now I want to update the Cinder scheduler. So it's all about being
comfortable, and having things be convenient for the operator. So the
RPC compatibility layer will allow the operator to do those upgrades
at their own pace.&lt;/p&gt;

&lt;p&gt;And, just like what we did with the Cinder database
upgrade case, we're trying to make this a generic solution so that we
can allow all OpenStack projects to take advantage of this same
solution.&lt;/p&gt;

&lt;p&gt;Very likely, just like what we did before, it will be something that
will surface up into the Oslo library as well.&lt;/p&gt;

&lt;p&gt;One feature that I know in particular that will be coming out, that
has already finished in the Liberty release, but we just didn't have
any of our volume drivers take advantage of it yet, is just some basic
replication ability. To be able to say that I have this volume, which
is the primary, and have other volumes that for that particular volume
will be replicated over. And if anything was to happen to the primary
volume, it would fail over to the secondary volume. So, really basic
replication, and we're not trying to do all the bells and whistles out
of the box right now, so it is a manual failover case. From what we've
got in previous summit discussions, people were kind of OK anyways
with doing manual failover. Weren't completely comfortable with
trusting software to do auto-failover. Eventually we'll get over
there, but I think getting all volume drivers over to supporting
replication is a big win.&lt;/p&gt;

&lt;p&gt;And then for the rest of the M release, I would say is a lot of
catchup with some of the other projects in terms of the ability - this
is not really something for the users, or the actual operators
themselves - but the support of microversions. Just having the ability
for us to progress forward in the API while still preserving some
backward compatibility.&lt;/p&gt;

&lt;p&gt;And then, the last part that I would say that may still not be
something noticeable to users and operators, but we are working a lot
more closely with the Nova team. In previous times there hasn't been a
lot of interactions between the two teams, and Nova uses the Cinder
API pretty heavily in the cases where persistent storage is needed.
And I would say that currently, today, we have quite a number of
issues that we're trying to work through in terms of how they're
expecting to use our API, and how well we defined our API to them
initially. So, I wouldn't really say it's a fault in anybody that's
been using our API, but more us trying to figure out how to define to
people that want to use our API how it should be used properly.&lt;br /&gt;
And now we're trying to clean that up as we go.&lt;/p&gt;

&lt;p&gt;So, basically, we should see in the M release is better interaction
between Nova and Cinder and, if there are any issues that happen in
those interactions, that they'll be able to recover from those issues.
So any time that Nova wants to create a volume and attach it, and
something were fail in between there, there would be some sort of
recovery that would happen, that the user can understand what just
happened, and could take action from there. Because currently, today,
there's really not much information for the user to go by to know what
to do next.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: Thank you very much for taking the time to do this.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;M&lt;/strong&gt;: No problem.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Haikel Guemar talks about RPM packaging</title>
    <link rel="alternate" href="http://rdoproject.org/blog/2015/10/haikel-guemar-talks-about-rpm-packaging/"/>
    <id>http://rdoproject.org/blog/2015/10/haikel-guemar-talks-about-rpm-packaging/</id>
    <published>2015-10-05T15:38:27+02:00</published>
    <updated>2016-06-09T17:28:37+02:00</updated>
    <author>
      <name>Rich Bowen</name>
    </author>
    <content type="html">&lt;p&gt;&lt;em&gt;This continues my &lt;a href="/blog/ptl-interviews"&gt;series talking with OpenStack project PTLs&lt;/a&gt; (Project Technical Leads) about their projects, what's new in Liberty, and what's coming in future releases.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If the audio player below doesn't work for you, you can listen to the interview &lt;a href="http://drbacchus.com/podcasts/openstack/haikel_packaging.mp3"&gt;HERE&lt;/a&gt; or
see the transcript below.&lt;/p&gt;

&lt;audio controls=""&gt;
  &lt;source src="http://drbacchus.com/podcasts/openstack/haikel_packaging.mp3" type="audio/mpeg" /&gt;
&lt;/audio&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: I'm Rich Bowen, I'm the community liaison for the OpenStack project
at Red Hat. I'm speaking with Haikel Guemar, who is the Project
Technical Lead (PTL) on the &lt;a href="https://wiki.openstack.org/wiki/Rpm-packaging"&gt;RPM packaging project&lt;/a&gt; at OpenStack. Thanks
for taking this time to speak with me.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;H&lt;/strong&gt;: Thank you, Rich. I appreciate it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: This is a fairly new project, in terms of actually being part of
OpenStack - is that right?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;H&lt;/strong&gt;: Yeah, exactly. We've been an official project for a few months, and
we're still in bootstrapping. There has been some discussion at the
last Summit to converge packaging project to work together, upstream.
After some discussion we discussion, we decided to split between two
separate projects, one around Debian packaging, and one around RPM
packaging. So we've been working with people from RDO, SUSE, and also
some people from Mirantis, to work together on RPM packaging upstream.&lt;/p&gt;

&lt;p&gt;By the way, I'm co-PTL of the project. I'm co-leading the project with
Dirk Mueller from SUSE. Hats off to Dirk, if you're listening to us.
We're trying to bring packaging into the heart of the project, because
OpenStack is fairly good at trying to develop its own software
pipeline, and the only bit that was missing was packaging - the last
missing bit between the project and the end-user.&lt;/p&gt;

&lt;p&gt;So, we've been trying to do that, and also help the upstream project
to understand what are our needs upstream, because we have very
specific requirements that we want them to hear.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: How do you picture this project changing the downstream projects like
RDO and Fuel? Do you think there will be a big impact on those, or
will they go on as though nothing's changed?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;H&lt;/strong&gt;: I think there will be some impact, because if all the downstream
packages are working on this, at some point we will be able to produce
multiple packagings. There will still be some differences, but what we
are working on - trying to mitigate them and trying to share most of
our workers.&lt;/p&gt;

&lt;p&gt;Speaking now with my RDO hat - I do hope that at some point, RDO will
derive from our upstream RPM packaging. That's what we want to do - be
closer to upstream. That's the point of RDO, not of the upstream
project.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: Looking forward to that time, what would then be the role of
projects like RDO, and Fuel. Would they still be relevant, or do you
see them going away long term.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;H&lt;/strong&gt;: Tough question. I think they will still be there, because at some
point we will be targeting very different segments. For instance, RDO
itself is the upstream for other downstream distributions of
OpenStack. Like, obviously, the Red Hat one, or Cisco's, or many
others. I think that maybe RDO might disappear, maybe, but we still
want to have some degree of integration. I hope that most of it will
go upstream, because that's where people are working, and we want to
deal with before it reaches the users, not after we release.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: In Liberty, and even more in future releases, with the integrated
release kind of going away, and more reliance on tags, is the RPM
packaging project going to package everything, or just a particular
tag, or will that be up to the community.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;H&lt;/strong&gt;: That will be up to the community. The core upstream RPM packaging
team will be working on core projects. We will be looking at Big Tent
projects according to our own drive. But if people want to contribute
any other Big Tent projects, they are welcome to do it. We are really
looking forward to extending the comunity. We're still a small team so
we will be focusing on core OpenStack. Currently we're working on
clients because that's what everyone needs to work with OpenStack, and
when we're done, then we'll start working on services.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: One of the things that RDO does is the CI effort within the CentOS
infrastructure. And there's obviously also a lot of that happening in
the upstream OpenStack. Will the RPM packaging rely on the upstream
for this?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;H&lt;/strong&gt;: We had some talks about it, and that was a funny discussion,
because we discovered that our respective CI are more or less the same
steps, and encounter the same issues. So we are willing to push more
of the CI upstream.&lt;/p&gt;

&lt;p&gt;I personally hope - but it's not something that we agreed on - I hope
that we will become part of the continuous delivery pipeline of
OpenStack. That means that every commit from upstream would be
packaged and then introduced into the CI, and then these patches would
be usable for other CI, since we don't have any concrete example, I
will take one from RDO. We've been working with Puppet upstream CI
recently, as they wanted to have come CI on CentOS, so we've been
working on fixing staging issues that were affecting their CI so they
could test properly Puppet upstream modules on CentOS. That was
amazing because they helped us find many many issues that we were able to
solve very early. And that's what I hope for the upstream RPM project.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: I noticed that they are no candidates for the PTL for the project
in the coming election. Is that just because the project is so new,
you're not going to have turnover in that?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;H&lt;/strong&gt;: Yeah, that was the discussion just this afternoon about it. We're
still new, and we don't want to confuse people with elections just
now. We're still building the community, and we've been working on
that with Dirk. The project won't be leaderless. It's still difficult
too, because we still have so much to do.&lt;/p&gt;

&lt;p&gt;The main difference between the RPM packaging team and other teams is
that we're not solely dedicated to work on upstream tasks, like
Cinder, Glance, or Nova. We have much more downstream-focused teams,
so we wanted to stay focused. So we thought we would just keep the
same PTLs for the next cycle. And then when we are more mature we will
have candidacy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: As PTL â€¦ it's been interesting talking with various projects,
because the role of PTL can vary a great deal depending on the size
and maturity of the project. I was wondering if you could tell us what
your responsibilities are as PTL.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;H&lt;/strong&gt;: Our responsibility is ensuring that all the community team is
aligned around the same goals. It means also trying to gather new
members. For instance, being the PTL brings the attention on you, so
people contact you so they can join the project. As we are still
small, that was a critical path, and I have been trying to have them
join the team, and make them active participants. I've been pretty
happy that, for instance, Mirantis people reached out to me, and
they've been able to join, and I appreciate that. One of the other
things is to ensure that we are working together. I'm pretty happy
that I am co-PTL with Dirk, because we can share the load. It is very
tiring. We have to schedule meetings and gather people. Recently we
had a hack day, to solve some of our impediments. We have to ensure
that we have enough people, and we have the right people, to help. For
instance, we have tooling issues, so I tried to get some people who
would be likely to work on that aspect available for that day. I do
not view the PTL as someone who decides everything. It's more about
being the coordinator.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: Finally: if somebody does want to get involved in the project,
where should they come? Is it to the mailing list, or IRC, or what?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;H&lt;/strong&gt;: We're still using the openstack-devel mailing list with 'RPM
Packaging' topics. We have an IRC channel on Freenode, which is
#openstack-rpm-packaging. We have regular meetings every two weeks on
Thursday. We sometimes have an additional meeting, at the same time.
We keep logs of our agenda. And if you need anything else, just ping
&lt;code&gt;number80&lt;/code&gt; or &lt;code&gt;dirk&lt;/code&gt; on the channel, and we'd be happy to help you.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt;: Thank you again for taking time to speak with me.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;H&lt;/strong&gt;: Thank you.&lt;/p&gt;
</content>
  </entry>
</feed>
